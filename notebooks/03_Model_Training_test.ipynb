{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm # Progession bar\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd().parent / 'data'\n",
    "# images_dir = data_dir / 'extracted_images' / 'images'\n",
    "imager_dir = 'C:/Users/RSCBAL04/Documents/images_1'\n",
    "metadata_dir = data_dir / 'Data_Entry_2017_v2020.csv'\n",
    "train_list_path = data_dir / 'train_val_list.txt'\n",
    "test_list_path = data_dir / 'test_list.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src') \n",
    "from model1 import JointLearningModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las imágenes disponibles en el directorio\n",
    "available_images = set(os.listdir(images_dir))\n",
    "\n",
    "\n",
    "def filter_and_save_list(list_path, output_path):\n",
    "    with open(list_path, 'r') as f:\n",
    "        images = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    filtered_images = [img for img in images if img in available_images]\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.writelines(f\"{img}\\n\" for img in filtered_images)\n",
    "    return filtered_images\n",
    "\n",
    "# Filtrar y guardar ambas listas\n",
    "filtered_train_list = filter_and_save_list(train_list_path, train_list_path)\n",
    "filtered_test_list = filter_and_save_list(test_list_path, test_list_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m     10\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),             \u001b[38;5;66;03m# Redimensionar a 224x224\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),                     \u001b[38;5;66;03m# Convertir a tensor\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m])  \u001b[38;5;66;03m# Normalización\u001b[39;00m\n\u001b[0;32m     14\u001b[0m ])\n\u001b[1;32m---> 17\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mChestXray8Dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiltered_train_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training mode\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create the DataLoader for training\u001b[39;00m\n\u001b[0;32m     26\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\RSCBAL04\\Documents\\GitHub\\JULE\\notebooks\\../src\\data_loader.py:13\u001b[0m, in \u001b[0;36mChestXray8Dataset.__init__\u001b[1;34m(self, img_dir, metadata_file, split_file, mode, transform)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(metadata_file)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     14\u001b[0m         split_list \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()]\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage Index\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(split_list)]\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "# Configuración del cuaderno\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from model1 import JointLearningModel\n",
    "from train import train_model\n",
    "from data_loader import ChestXray8Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),             # Redimensionar a 224x224\n",
    "    transforms.ToTensor(),                     # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalización\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ChestXray8Dataset(\n",
    "    img_dir=images_dir, \n",
    "    metadata_file=metadata_dir, \n",
    "    split_file=train_list_path,\n",
    "    mode='train',  # Training mode\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create the DataLoader for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "# Inicializar el modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = JointLearningModel(num_clusters=14, embedding_dim=128, pretrained=True).to(device)\n",
    "\n",
    "# Definir optimizador y función de pérdida\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings, cluster_probs = model(images)\n",
    "\n",
    "        # Clustering requiere logits\n",
    "        loss = criterion(cluster_probs, labels.argmax(dim=1))  # one-hot\n",
    "\n",
    "        # Backward pass y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marta_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
